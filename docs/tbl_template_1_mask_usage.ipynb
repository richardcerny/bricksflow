{"cells":[{"cell_type":"markdown","source":["<img src=\"https://github.com/richardcerny/bricksflow/raw/rc-bricksflow2.1/docs/img/databricks_icon.png?raw=true\" width=100/>\n# Bricksflow example 1.\n\n## Create new table from CSV\n\nThis is a very first template notebook that should give you brief overeview how to develop pipeline using Bricksflow.\n\nYou learn how to organize cells and functions, use `@decorators`, pass variables from `config.yaml`.\n\nThere are other template notebooks within this project so just look for _template_ notebooks within workspace."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e29f9242-4f18-4ea0-8099-29f8b0a2de1d"}}},{"cell_type":"markdown","source":["### Requirements for running this notebook\nIt is possible to run this demo notebook to see it in action. The datasource is public dataset from Databricks so you should be able to access it.\n\nIt is expected that following set-up is already configured.\n\n##### Environment variables defined on a cluster\n```\nAPP_ENV=dev\n```\n\n##### Database `dev_bronze_covid`\n\nIf you want to run this notebook you need to create it using this command(the cell is prepared bellow):\n```\n%sql\ncreate database if not exists dev_bronze_covid\n```\n__NOTE:__ Tested on a cluster running Databricks 7.3."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e690abf7-bbf9-46d2-97a1-4b00c212e627"}}},{"cell_type":"code","source":["%sql\n-- this cell is only for demo purposes\ncreate database if not exists dev_bronze_covid;\ncreate database if not exists dev_silver_covid;\ncreate database if not exists dev_gold_reporting"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2755f1ee-b535-4e73-966b-5e84912d48ff"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":[],"pivotAggregation":null,"xColumns":[],"yColumns":[]},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### This command loads Bricksflow framework and its dependencies"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8858fdf6-4f99-4699-a1fa-3dc7feb9a8b9"}}},{"cell_type":"code","source":["%run ../../../app/install_master_package"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"31f18c2f-e3a2-4264-bbb4-5106831084fd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### All your imports should be placed up here"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a5018330-2dff-4cd1-851f-afe6a35b740d"}}},{"cell_type":"code","source":["from datetime import datetime\nfrom pyspark.sql import functions as F\n\nfrom logging import Logger\nfrom datalakebundle.table.TableManager import TableManager\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.dataframe import DataFrame\nfrom databricksbundle.notebook.decorators import dataFrameLoader, transformation, dataFrameSaver\nfrom datalakebundle.table.TableNames import TableNames"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2f9aad51-ed9b-4d4d-ad10-b8b03af204b7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Cells and functions\n\nBricksflow`s best practice to write transformation is by using function per cell approach. Each transformation has its own function and is used in one cell. This sorting of cells and functions significatly improves debuggability of each step and bring other advantages.\n\nWe try to avoid complex dataframe manipulation within one function. Function name should briefly describe what it does.\n\nWe are able to create so called *Lineage* that shows all aggregations input/output tables. This is usefull especially for business analysts as they have better idead what is happening.\n\n#### Lineage example\n\n<img src=\"https://github.com/richardcerny/bricksflow/raw/rc-bricksflow2.1/docs/img/lineage.png?raw=true\" width=1200/>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9daf0ab8-66ac-4e71-8810-ec2b098869c3"}}},{"cell_type":"code","source":["# Check \n@dataFrameLoader(\"%datalakebundle.tables%\", display=False)\ndef read_csv_mask_usage(parameters_datalakebundle, spark: SparkSession, logger: Logger):\n    source_csv_path = parameters_datalakebundle['bronze_covid.tbl_template_1_mask_usage']['params']['source_csv_path']\n    logger.info(f\"Reading CSV from source path: `{source_csv_path}`.\")\n    return (\n        spark\n            .read\n            .format('csv')\n            .option('header', 'true')\n            .option('inferSchema', 'true') # Tip: it might be better idea to define schema!\n            .load(source_csv_path)\n            .limit(10) # only for test\n    )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7631190f-ee20-4377-9d6b-230b18d127c6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"read_csv_mask_usage_df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"COUNTYFP","nullable":true,"type":"integer"},{"metadata":{},"name":"NEVER","nullable":true,"type":"double"},{"metadata":{},"name":"RARELY","nullable":true,"type":"double"},{"metadata":{},"name":"SOMETIMES","nullable":true,"type":"double"},{"metadata":{},"name":"FREQUENTLY","nullable":true,"type":"double"},{"metadata":{},"name":"ALWAYS","nullable":true,"type":"double"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"><span class=\"ansi-green-fg\">09:32:14 INFO - Reading CSV from source path: `dbfs:/databricks-datasets/COVID/covid-19-data/mask-use/mask-use-by-county.csv`.\n{dbName: dev_bronze_covid, tableIdentifier: tbl_template_1_mask_usage, dbIdentifier: bronze_covid, tableName: tbl_template_1_mask_usage}</span>\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-green-fg\">09:32:14 INFO - Reading CSV from source path: `dbfs:/databricks-datasets/COVID/covid-19-data/mask-use/mask-use-by-county.csv`.\n{dbName: dev_bronze_covid, tableIdentifier: tbl_template_1_mask_usage, dbIdentifier: bronze_covid, tableName: tbl_template_1_mask_usage}</span>\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### @decorators\nDid you notice that peace of code above a function starting with \"@\". It`s a standard python element called _decorator_. Bricksflow uses decorators to enable software engineering approaches while using advantage of interactive notebook. Run a function without explicitly calling it - simulates interactive cell and allows to run as a script without any modification. It is possible to generate Lineage documenation based on order of transformations and other things and many in the future.\n- *@dataFrameLoader* - use when loading table or data from source. Accepts varibles from config and returns dataframe.\n- *@transformation* - use for any kind of dataframe transformation/step. You probably use many of those. Accepts Input dataframe and varibles from config, Returns dataframe.\n- *@dataFrameSaver* - use when saving dataframe to a table. Accepts only Input dataframe and varibles from config.\n- *@notebookFunction* - use when running any other Python code like - Mlflow, Widgets, Secrets,...\n\n#### Decorators parameters\nIt is possible to define some functionality by decorates. You have this possibilities:\n- Variables from config -> see section _Define param in config.yaml_ bellow \n- `display=True/False`\n  Do you use display(df) function to show content of a dataframe? This parameter is exactly the same. By using it as decorator param we are able to easily deactivate it in production where it is not necessary. Set the parameter to True to show data preview or False to skip preview.\n  \n  <img src=\"https://github.com/richardcerny/bricksflow/raw/rc-bricksflow2.1/docs/img/display_true.png?raw=true\" width=800/>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"28e99a13-f460-441f-9d94-97724c8c9123"}}},{"cell_type":"markdown","source":["### Set parameter display=True to show results in this cell"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6950ac2a-6750-4767-bbac-40099c39c873"}}},{"cell_type":"code","source":["@transformation(read_csv_mask_usage, display=True)\ndef add_column_insert_ts(df: DataFrame, logger: Logger):\n    logger.info(\"Adding Insert timestamp\")\n    return df.withColumn('INSERT_TS', F.lit(datetime.now()))\n    "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf07337a-96b3-4ae2-b162-67e92e122553"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[{"name":"add_column_insert_ts_df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"COUNTYFP","nullable":true,"type":"integer"},{"metadata":{},"name":"NEVER","nullable":true,"type":"double"},{"metadata":{},"name":"RARELY","nullable":true,"type":"double"},{"metadata":{},"name":"SOMETIMES","nullable":true,"type":"double"},{"metadata":{},"name":"FREQUENTLY","nullable":true,"type":"double"},{"metadata":{},"name":"ALWAYS","nullable":true,"type":"double"},{"metadata":{},"name":"INSERT_TS","nullable":false,"type":"timestamp"}],"type":"struct"},"tableIdentifier":null}],"data":[[1001,0.053,0.074,0.134,0.295,0.444,"2021-01-13T09:32:20.557+0000"],[1003,0.083,0.059,0.098,0.323,0.436,"2021-01-13T09:32:20.557+0000"],[1005,0.067,0.121,0.12,0.201,0.491,"2021-01-13T09:32:20.557+0000"],[1007,0.02,0.034,0.096,0.278,0.572,"2021-01-13T09:32:20.557+0000"],[1009,0.053,0.114,0.18,0.194,0.459,"2021-01-13T09:32:20.557+0000"],[1011,0.031,0.04,0.144,0.286,0.5,"2021-01-13T09:32:20.557+0000"],[1013,0.102,0.053,0.257,0.137,0.451,"2021-01-13T09:32:20.557+0000"],[1015,0.152,0.108,0.13,0.167,0.442,"2021-01-13T09:32:20.557+0000"],[1017,0.117,0.037,0.15,0.136,0.56,"2021-01-13T09:32:20.557+0000"],[1019,0.135,0.027,0.161,0.158,0.52,"2021-01-13T09:32:20.557+0000"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":[],"pivotAggregation":null,"xColumns":[],"yColumns":[]},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"COUNTYFP","type":"\"integer\"","metadata":"{}"},{"name":"NEVER","type":"\"double\"","metadata":"{}"},{"name":"RARELY","type":"\"double\"","metadata":"{}"},{"name":"SOMETIMES","type":"\"double\"","metadata":"{}"},{"name":"FREQUENTLY","type":"\"double\"","metadata":"{}"},{"name":"ALWAYS","type":"\"double\"","metadata":"{}"},{"name":"INSERT_TS","type":"\"timestamp\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>COUNTYFP</th><th>NEVER</th><th>RARELY</th><th>SOMETIMES</th><th>FREQUENTLY</th><th>ALWAYS</th><th>INSERT_TS</th></tr></thead><tbody><tr><td>1001</td><td>0.053</td><td>0.074</td><td>0.134</td><td>0.295</td><td>0.444</td><td>2021-01-13T09:32:20.557+0000</td></tr><tr><td>1003</td><td>0.083</td><td>0.059</td><td>0.098</td><td>0.323</td><td>0.436</td><td>2021-01-13T09:32:20.557+0000</td></tr><tr><td>1005</td><td>0.067</td><td>0.121</td><td>0.12</td><td>0.201</td><td>0.491</td><td>2021-01-13T09:32:20.557+0000</td></tr><tr><td>1007</td><td>0.02</td><td>0.034</td><td>0.096</td><td>0.278</td><td>0.572</td><td>2021-01-13T09:32:20.557+0000</td></tr><tr><td>1009</td><td>0.053</td><td>0.114</td><td>0.18</td><td>0.194</td><td>0.459</td><td>2021-01-13T09:32:20.557+0000</td></tr><tr><td>1011</td><td>0.031</td><td>0.04</td><td>0.144</td><td>0.286</td><td>0.5</td><td>2021-01-13T09:32:20.557+0000</td></tr><tr><td>1013</td><td>0.102</td><td>0.053</td><td>0.257</td><td>0.137</td><td>0.451</td><td>2021-01-13T09:32:20.557+0000</td></tr><tr><td>1015</td><td>0.152</td><td>0.108</td><td>0.13</td><td>0.167</td><td>0.442</td><td>2021-01-13T09:32:20.557+0000</td></tr><tr><td>1017</td><td>0.117</td><td>0.037</td><td>0.15</td><td>0.136</td><td>0.56</td><td>2021-01-13T09:32:20.557+0000</td></tr><tr><td>1019</td><td>0.135</td><td>0.027</td><td>0.161</td><td>0.158</td><td>0.52</td><td>2021-01-13T09:32:20.557+0000</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Passing dataframe between functions\nNormally you would pass dataframes between tranformation like this:\n```\ndf_1 = df2.select('xxx',...)\ndf2 = df3.withColumn(...\ndf3.write...\n```\n*Bricksflow does it a bit differently!*\n\nBasically you use name of original function and place it as an input parameter to following(or any other) function`s @decorator. Thanks to this you are able to easilly navigate between functions in your IDE.\nSee bellow how to pass dataframe from one function to another.\n\n![Passing dataframe between functions](https://github.com/richardcerny/bricksflow/raw/rc-bricksflow2.1/docs/img/df_passing.png)\n\nYou can see this in acion accross this notebook."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"828b60a0-7938-4622-9d8e-b82e024f1446"}}},{"cell_type":"code","source":["@dataFrameSaver(add_column_insert_ts)\ndef save_table_bronze_covid_tbl_template_1_mask_usage(df: DataFrame, logger: Logger, tableNames: TableNames,  tableManager: TableManager):\n    \n    # Recreate = remove table and create again\n    tableManager.recreate('bronze_covid.tbl_template_1_mask_usage')\n    \n    outputTableName = tableNames.getByAlias('bronze_covid.tbl_template_1_mask_usage')\n    logger.info(f\"Saving data to table: {outputTableName}\")\n    (\n        df\n            .select(\n                'COUNTYFP',\n                'NEVER',\n                'RARELY',\n                'SOMETIMES',\n                'FREQUENTLY',\n                'ALWAYS',\n                'INSERT_TS'\n            )\n            .write\n            .option('partitionOverwriteMode', 'dynamic')\n            .insertInto(outputTableName)\n    )\n    logger.info(f\"Data successfully saved to: {outputTableName}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"adcbf465-dabf-4ef3-ae7f-7b9785b46f59"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"><span class=\"ansi-green-fg\">09:32:21 INFO - Deleting Hive table dev_bronze_covid.tbl_template_1_mask_usage\n{dbName: dev_bronze_covid, tableIdentifier: tbl_template_1_mask_usage, dbIdentifier: bronze_covid, tableName: tbl_template_1_mask_usage}</span>\n<span class=\"ansi-green-fg\">09:32:22 INFO - Hive table dev_bronze_covid.tbl_template_1_mask_usage deleted\n{dbName: dev_bronze_covid, tableIdentifier: tbl_template_1_mask_usage, dbIdentifier: bronze_covid, tableName: tbl_template_1_mask_usage}</span>\n<span class=\"ansi-green-fg\">09:32:22 INFO - Deleting HDFS files from /dev/bronze/covid/tbl_template_1_mask_usage.delta\n{dbName: dev_bronze_covid, tableIdentifier: tbl_template_1_mask_usage, dbIdentifier: bronze_covid, tableName: tbl_template_1_mask_usage}</span>\n<span class=\"ansi-green-fg\">09:32:22 INFO - HDFS files deleted from /dev/bronze/covid/tbl_template_1_mask_usage.delta\n{dbName: dev_bronze_covid, tableIdentifier: tbl_template_1_mask_usage, dbIdentifier: bronze_covid, tableName: tbl_template_1_mask_usage}</span>\n<span class=\"ansi-green-fg\">09:32:22 INFO - Recreating Hive table dev_bronze_covid.tbl_template_1_mask_usage (existed before)\n{dbName: dev_bronze_covid, tableIdentifier: tbl_template_1_mask_usage, dbIdentifier: bronze_covid, tableName: tbl_template_1_mask_usage}</span>\n<span class=\"ansi-green-fg\">09:32:26 INFO - Saving data to table: dev_bronze_covid.tbl_template_1_mask_usage\n{dbName: dev_bronze_covid, tableIdentifier: tbl_template_1_mask_usage, dbIdentifier: bronze_covid, tableName: tbl_template_1_mask_usage}</span>\n<span class=\"ansi-green-fg\">09:32:28 INFO - Data successfully saved to: dev_bronze_covid.tbl_template_1_mask_usage\n{dbName: dev_bronze_covid, tableIdentifier: tbl_template_1_mask_usage, dbIdentifier: bronze_covid, tableName: tbl_template_1_mask_usage}</span>\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-green-fg\">09:32:21 INFO - Deleting Hive table dev_bronze_covid.tbl_template_1_mask_usage\n{dbName: dev_bronze_covid, tableIdentifier: tbl_template_1_mask_usage, dbIdentifier: bronze_covid, tableName: tbl_template_1_mask_usage}</span>\n<span class=\"ansi-green-fg\">09:32:22 INFO - Hive table dev_bronze_covid.tbl_template_1_mask_usage deleted\n{dbName: dev_bronze_covid, tableIdentifier: tbl_template_1_mask_usage, dbIdentifier: bronze_covid, tableName: tbl_template_1_mask_usage}</span>\n<span class=\"ansi-green-fg\">09:32:22 INFO - Deleting HDFS files from /dev/bronze/covid/tbl_template_1_mask_usage.delta\n{dbName: dev_bronze_covid, tableIdentifier: tbl_template_1_mask_usage, dbIdentifier: bronze_covid, tableName: tbl_template_1_mask_usage}</span>\n<span class=\"ansi-green-fg\">09:32:22 INFO - HDFS files deleted from /dev/bronze/covid/tbl_template_1_mask_usage.delta\n{dbName: dev_bronze_covid, tableIdentifier: tbl_template_1_mask_usage, dbIdentifier: bronze_covid, tableName: tbl_template_1_mask_usage}</span>\n<span class=\"ansi-green-fg\">09:32:22 INFO - Recreating Hive table dev_bronze_covid.tbl_template_1_mask_usage (existed before)\n{dbName: dev_bronze_covid, tableIdentifier: tbl_template_1_mask_usage, dbIdentifier: bronze_covid, tableName: tbl_template_1_mask_usage}</span>\n<span class=\"ansi-green-fg\">09:32:26 INFO - Saving data to table: dev_bronze_covid.tbl_template_1_mask_usage\n{dbName: dev_bronze_covid, tableIdentifier: tbl_template_1_mask_usage, dbIdentifier: bronze_covid, tableName: tbl_template_1_mask_usage}</span>\n<span class=\"ansi-green-fg\">09:32:28 INFO - Data successfully saved to: dev_bronze_covid.tbl_template_1_mask_usage\n{dbName: dev_bronze_covid, tableIdentifier: tbl_template_1_mask_usage, dbIdentifier: bronze_covid, tableName: tbl_template_1_mask_usage}</span>\n</div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"tbl_template_1_mask_usage","dashboards":[],"language":"python","widgets":{},"notebookOrigID":397401967460296}},"nbformat":4,"nbformat_minor":0}
